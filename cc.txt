import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer  # Import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np  # Import numpy

# --- 1. Data Preparation for Classification ---
# a. Using Preprocessed Data
# Assuming you have a preprocessed DataFrame called 'df'
# Sample Data (replace with your actual data loading)
data = {
    'Make': ['FORD', 'BMW', 'TOYOTA', 'FORD', 'BMW', 'AUDI', 'TOYOTA', 'BMW', 'FORD', 'AUDI'],
    'Model': ['Focus', 'X5', 'Camry', 'Fiesta', 'X3', 'A4', 'Corolla', 'X1', 'Fusion', 'A6'],
    'Fuel_Type': ['Gasoline', 'Diesel', 'Gasoline', 'Gasoline', 'Diesel', 'Gasoline', 'Hybrid', 'Diesel', 'Gasoline', 'Diesel'],
    'Transmission': ['Manual', 'Automatic', 'Automatic', 'Manual', 'Automatic', 'Automatic', 'Automatic', 'Manual', 'Automatic', 'Automatic'],
    'Car_Type': ['Sedan', 'SUV', 'Sedan', 'Hatchback', 'SUV', 'Sedan', 'Sedan', 'SUV', 'Sedan', 'Sedan'],
    'Price': [25000, 40000, 22000, 18000, 35000, 30000, 28000, 32000, 26000, 42000],
    'Kilometer': [50000, 30000, 60000, 70000, 40000, 55000, 45000, 35000, 65000, 25000],
    'Warranty': [True, False, True, False, True, False, True, False, True, False],
    'Quality_Score': [7.5, 8.2, 6.8, 7.0, 8.0, 7.2, 7.8, 8.5, 6.5, 8.8],
    'Car_Age': [5, 3, 7, 6, 4, 5, 2, 3, 8, 1],
    'Description': ['Great condition', 'Luxury SUV', 'Reliable car', 'Fun to drive', 'Well maintained',
                    'Sporty sedan', 'Eco-friendly', 'Compact SUV', 'Spacious sedan', 'Executive car']
}
df = pd.DataFrame(data)

# Introduce some NaN values for demonstration
df.loc[0, 'Quality_Score'] = np.nan
df.loc[3, 'Kilometer'] = np.nan

# b. Target Variable Creation (Suitability)
# Example rule: Recommended if Price and Kilometer are below their medians
price_median = df['Price'].median()
kilometer_median = df['Kilometer'].median()

# Create the 'Suitability' target variable
df['Suitability'] = ((df['Price'] < price_median) & (df['Kilometer'] < kilometer_median)).astype(int)

# Separate features (X) and target (y)
X = df.drop('Suitability', axis=1)
y = df['Suitability']

# Identify categorical columns
categorical_features = X.select_dtypes(include='object').columns.tolist()

# --- 2. Model Selection and Implementation ---
# a. Choice of Algorithms (Decision Tree, Random Forest, Logistic Regression)

# b. Training Process
# Data Splitting
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create a ColumnTransformer to apply one-hot encoding and imputation
# Impute missing values with the mean
numeric_features = X.select_dtypes(include=['number']).columns.tolist()
transformers = []
if categorical_features:
    transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features))
if numeric_features:
    transformers.append(('num', SimpleImputer(strategy='mean'), numeric_features))

transformer = ColumnTransformer(
    transformers=transformers,
    remainder='passthrough'  # Keep other columns as they are
)

# Instantiate the classifiers
dt_model = DecisionTreeClassifier(random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
lr_model = LogisticRegression(max_iter=1000)

# Create pipelines that first transform the data and then train the model
dt_pipeline = Pipeline(steps=[('transformer', transformer), ('classifier', dt_model)])
rf_pipeline = Pipeline(steps=[('transformer', transformer), ('classifier', rf_model)])
lr_pipeline = Pipeline(steps=[('transformer', transformer), ('classifier', lr_model)])

# Train the models using the training data
dt_pipeline.fit(X_train, y_train)
rf_pipeline.fit(X_train, y_train)
lr_pipeline.fit(X_train, y_train)

# --- 3. Evaluation of the Classifiers ---
# a. Metrics Used (Accuracy, Precision, Recall, F1 Score, Confusion Matrix)

# Make predictions on the test set
y_pred_dt = dt_pipeline.predict(X_test)
y_pred_rf = rf_pipeline.predict(X_test)
y_pred_lr = lr_pipeline.predict(X_test)

# Calculate metrics for each model
def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    confusion = confusion_matrix(y_true, y_pred)

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("Confusion Matrix:\n", confusion)
    print("-" * 30)

# Evaluate each model
print("Evaluation Metrics:")
evaluate_model(y_test, y_pred_dt, "Decision Tree")
evaluate_model(y_test, y_pred_rf, "Random Forest")
evaluate_model(y_test, y_pred_lr, "Logistic Regression")

# --- 4. Integration with the Recommendation Engine ---
# a. Filtering Stage (Pre-filtering, Role of Probability Scores)
# Example: Use the Random Forest model to predict suitability and filter the data
# In a real application, this would be integrated into your recommendation engine

# Get probability scores from Logistic Regression (example)
y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]  # Probability of being "Recommended"

# Filter based on a probability threshold (e.g., 0.7)
threshold = 0.7
recommended_indices = np.where(y_proba_lr > threshold)[0]
recommended_cars = X_test.iloc[recommended_indices]

print("\nRecommended Cars (based on Logistic Regression probability > 0.7):")
print(recommended_cars)

# b. Hybrid Recommendation Approach (Content-Based Ranking, Final Recommendations)
# This part would involve integrating the filtered cars with your content-based
# recommendation engine, which is beyond the scope of this code snippet.

# --- 5. Continuous Improvement ---
# a. User Feedback Loop (Feedback Collection, Model Refinement)
# b. Monitoring and Updates (Performance Monitoring, Regular Updates)
# These steps involve setting up a system to collect user feedback and
# automatically retrain the model, which is a more complex process and
# depends on your specific infrastructure.
