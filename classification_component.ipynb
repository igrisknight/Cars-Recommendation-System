{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "       id        company               model           variant fueltype  \\\n",
      "0  555675  MARUTI SUZUKI  CELERIO(2017-2019)     1.0 ZXI AMT O   PETROL   \n",
      "1  556383  MARUTI SUZUKI                ALTO               LXI   PETROL   \n",
      "2  556422        HYUNDAI           GRAND I10    1.2 KAPPA ASTA   PETROL   \n",
      "3  556771           TATA               NEXON           XT PLUS   PETROL   \n",
      "4  559619           FORD                FIGO  EXI DURATORQ 1.4   DIESEL   \n",
      "\n",
      "   colour  kilometer manufacturedate  modelyear  price  ...  \\\n",
      "0  Silver  -0.692907      01-02-2018       2018    NaN  ...   \n",
      "1     Red  -1.542958      01-03-2021       2021    NaN  ...   \n",
      "2    Grey  -0.518549      01-03-2015       2015    NaN  ...   \n",
      "3    Blue  -1.439502      01-08-2020       2020    NaN  ...   \n",
      "4  Silver   1.960999      01-11-2010       2010    NaN  ...   \n",
      "\n",
      "  transmissiontype_m  transmissiontype_manual  transmissiontype_manual  \\\n",
      "0              False                    False                    False   \n",
      "1              False                    False                     True   \n",
      "2              False                    False                     True   \n",
      "3              False                    False                    False   \n",
      "4              False                    False                     True   \n",
      "\n",
      "   transmissiontype_unknown  transmissiontype_manual  cngkit_company fitted  \\\n",
      "0                      True                    False                  False   \n",
      "1                     False                    False                  False   \n",
      "2                     False                    False                  False   \n",
      "3                      True                    False                  False   \n",
      "4                     False                    False                  False   \n",
      "\n",
      "   cngkit_unknown  owner_2nd owner  owner_3rd owner  owner_4th owner  \n",
      "0            True            False            False            False  \n",
      "1            True            False            False            False  \n",
      "2            True            False            False            False  \n",
      "3            True            False            False            False  \n",
      "4            True             True            False            False  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset from a CSV file\n",
    "df = pd.read_csv(\"preprocess_used_cars.csv\")\n",
    "\n",
    "# Standardize column names (convert to lowercase)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitability distribution:\n",
      "suitability\n",
      "0    547\n",
      "1    517\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Create target variable 'suitability' if it doesn't already exist.\n",
    "# Example rule: A car is recommended (1) if its price is below the median OR \n",
    "# (if qualityscore exists) its qualityscore is above the median.\n",
    "if 'suitability' not in df.columns:\n",
    "    price_median = df['price'].median()\n",
    "    if 'qualityscore' in df.columns:\n",
    "        qualityscore_median = df['qualityscore'].median()\n",
    "        df['suitability'] = ((df['price'] < price_median) | (df['qualityscore'] > qualityscore_median)).astype(int)\n",
    "    else:\n",
    "        df['suitability'] = (df['price'] < price_median).astype(int)\n",
    "\n",
    "# Print the distribution of the target variable\n",
    "print(\"Suitability distribution:\")\n",
    "print(df['suitability'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features: ['company', 'model', 'variant', 'fueltype', 'colour', 'manufacturedate', 'dealerstate', 'dealername', 'city']\n",
      "Numerical Features: ['id', 'kilometer', 'modelyear', 'price', 'warranty', 'qualityscore']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the features DataFrame (X) by dropping the target column,\n",
    "# and set y as the target variable.\n",
    "X = df.drop('suitability', axis=1)\n",
    "y = df['suitability']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Categorical Features:\", categorical_features)\n",
    "print(\"Numerical Features:\", numerical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformers for numerical and categorical features\n",
    "\n",
    "# Numeric transformer: impute missing values with the mean\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Categorical transformer: one-hot encode categorical variables.\n",
    "# (Using 'sparse_output=False' for scikit-learn 1.2+)\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Combine the transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test set sizes:\n",
      "X_train: (851, 38) X_test: (213, 38)\n"
     ]
    }
   ],
   "source": [
    "# Define classifiers with a fixed random_state for reproducibility\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create pipelines for each model (preprocessor + classifier)\n",
    "dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', dt_model)])\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', rf_model)])\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', lr_model)])\n",
    "\n",
    "# Split data into training and test sets (using stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training and Test set sizes:\")\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['price']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'classifier__C': 100, 'classifier__solver': 'lbfgs'}\n",
      "Best cross-validation F1 score (Logistic Regression): 0.9773407216986041\n"
     ]
    }
   ],
   "source": [
    "# Define a parameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['lbfgs', 'saga']  # Adjust based on your requirements\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for Logistic Regression pipeline\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=lr_pipeline,\n",
    "    param_grid=param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search on the training data\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Logistic Regression:\", grid_search_lr.best_params_)\n",
    "print(\"Best cross-validation F1 score (Logistic Regression):\", grid_search_lr.best_score_)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_lr = grid_search_lr.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression (Tuned)\n",
      "Accuracy: 0.9765\n",
      "Precision: 0.9623\n",
      "Recall: 0.9903\n",
      "F1 Score: 0.9761\n",
      "Confusion Matrix:\n",
      "[[106   4]\n",
      " [  1 102]]\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['price']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set using the best Logistic Regression pipeline\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "\n",
    "# Define a function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate the tuned Logistic Regression model\n",
    "evaluate_model(y_test, y_pred_lr, \"Logistic Regression (Tuned)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Component training is complete. The trained pipeline and candidate data are ready for use.\n"
     ]
    }
   ],
   "source": [
    "# The function below encapsulates the entire process for modularity\n",
    "def train_classification_component(data_path=\"preprocess_used_cars.csv\"):\n",
    "    # Load, preprocess, create target, split data, and tune model (using cells 2-8)\n",
    "    # (The code from Cells 2 through 8 would be integrated here)\n",
    "    # For brevity, we assume the operations have been executed as above.\n",
    "    # Return the best tuned Logistic Regression pipeline and the candidate DataFrame (X)\n",
    "    return best_lr, X\n",
    "\n",
    "# When running the notebook, you can now call this function at the end.\n",
    "trained_pipeline, candidate_data = train_classification_component()\n",
    "print(\"Classification Component training is complete. The trained pipeline and candidate data are ready for use.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
